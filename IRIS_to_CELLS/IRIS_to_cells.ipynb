{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import networkx as nx\n",
    "import shapely.wkt\n",
    "\n",
    "# Useful functions\n",
    "def make_grid(shapefile, deltax=None, deltay=None):\n",
    "    \"\"\"\n",
    "    deltax & deltay should be measure in meters. standar values: 2000,1000,500\n",
    "    for EU reglamentations\n",
    "    \"\"\"\n",
    "    xmin, ymin, xmax, ymax = shapefile.total_bounds # extraigo el bounding box del SHAPEFILE\n",
    "    if deltax == deltay == None:\n",
    "        # # # # ####################### INI: esta parte es para pruebas ###################\n",
    "        # # # # # el verdadero dx y dy se definiran en metros segun los arg de entrada\n",
    "        ncajas = 10                     # defino una cant de cajas para ir jugando\n",
    "        deltax = (xmax-xmin)/ncajas         # defino el paso X simple solo para ir jugando\n",
    "        deltay = (ymax-ymin)/ncajas         # defino el paso Y simple solo para ir jugando\n",
    "        # # # # ####################### FIN: esta parte es de entrenamiento ###################\n",
    "    cols = np.arange(int(np.floor(xmin)), int(np.ceil(xmax)), deltax)\n",
    "    rows = np.arange(int(np.floor(ymin)), int(np.ceil(ymax)), deltay)\n",
    "    poligonos = []\n",
    "    for x in cols:\n",
    "        for y in rows:\n",
    "            poligonos.append(Polygon([ (x,y), (x+deltax,y), (x+deltax,y+deltay), (x,y+deltay) ]))\n",
    "    rejilla = gpd.GeoDataFrame({'geometry':poligonos}, crs=shapefile.crs)\n",
    "    return rejilla\n",
    "\n",
    "def my_weight(G, u, v, weight=\"weight\"):\n",
    "    \"\"\"\n",
    "    fx que toma un grafo G BIPARTITO.\n",
    "    Toma dos nodos (u,v) del layer donde hace la proyeccion.\n",
    "    Toma los pesos de los links con los que u y v se conectan a los vecinos del otro layer.\n",
    "    Identifica los vecinos comunes que tengan u y v en el otro layer.\n",
    "    Suma el producto de los pesos de u y v con sus vecinos comunes.\n",
    "    Normaliza esta suma con el producto la suma de TODOS los pesos de u y v con los vecinos del otro layer\n",
    "    Crea un denominador de normalización como la suma los cocientes de la suma de los cuadraddos\n",
    "    de los TODOS los pesos de u y v con los vecinos en el otro layer, respecto al cuadrado\n",
    "    de TODOS los pesos de u y v con los vecinos en el otro layer, multiplicado por 0.5.\n",
    "    Entrega un peso (w_uv) entre (u,v) tal que es una normalización explicada abajo\n",
    "    \"\"\"\n",
    "    \n",
    "    pisos_U = list(dict(G[u]).values()) # pesos de U con los vecinos del otro layer\n",
    "    pisos_V = list(dict(G[v]).values()) # pesos de V con los vecinos del otro layer\n",
    "    \n",
    "    npisos_U = sum( c.get('weight') for c in pisos_U )  # suma de los pesos de los vecinos de U\n",
    "    npisos_V = sum( c.get('weight') for c in pisos_V )  # suma de los pesos de los vecinos de V\n",
    "    \n",
    "    w = 0               # suma del prod de pesos con vecinos COMUNES  en el otro layer de (U,V)\n",
    "    for nbr in set(G[u]) & set(G[v]):                   # INTERSECCION de vecinos de (U,V)\n",
    "        w += G[u][nbr].get(weight) * G[v][nbr].get(weight)\n",
    "\n",
    "    numerador = w/(npisos_U*npisos_V)                   # numerador de la ecuacion de pesos\n",
    "    \n",
    "    sum_U_2 = sum( c.get('weight')**2 for c in pisos_U )/npisos_U**2 # suma de cuadrados de pesos de U\n",
    "    sum_V_2 = sum( c.get('weight')**2 for c in pisos_V )/npisos_V**2 # suma de cuadrados de pesos de v\n",
    "    \n",
    "    denominador = 0.5*(sum_U_2 + sum_V_2)               # denomminador de la ecuacion de pesos\n",
    "\n",
    "    w_uv = numerador/denominador   # propuesta del peso entre (u,v)\n",
    "    \n",
    "    # w_uv_malos = 0                 # pesos mayores que uno\n",
    "    # if w_uv > 1:\n",
    "    #     w_uv_malos += w_uv_malos\n",
    "    return w_uv\n",
    "\n",
    "def grid_census(shp,cell_size):\n",
    "    grid = make_grid(shp[[\"geometry\"]].dissolve(),deltax=cell_size,deltay=cell_size)\n",
    "    grid = gpd.sjoin(grid,shp[[\"geometry\"]].dissolve(), how='inner').reset_index().drop(columns=[\"index\",\"index_right\"]).reset_index()\n",
    "    grid = grid.rename(columns={\"index\":\"index_cell\"})\n",
    "\n",
    "    # We intersect the grid with the census\n",
    "    grid_cens = gpd.overlay(grid,shp,how='intersection')\n",
    "    \n",
    "    # We create a new column with the area of each census\n",
    "    grid_cens[\"Area\"] = grid_cens.geometry.area\n",
    "\n",
    "    # Normalize the area according to the census\n",
    "    df = grid_cens.groupby(\"CODE_IRIS\").sum().reset_index()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        grid_cens.loc[grid_cens[\"CODE_IRIS\"] == df[\"CODE_IRIS\"][i],\"Area\"] = grid_cens.loc[grid_cens[\"CODE_IRIS\"] == df[\"CODE_IRIS\"][i],\"Area\"]/df[\"Area\"][i]\n",
    "\n",
    "    return grid_cens   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitio = \"Paris\"   # Sitio de estudio (Paris, Toulouse, Marseille)\n",
    "cell_size = 1000  # Tamaño de celda en metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data of the FUA\n",
    "iris = pd.read_csv('./zonage/'+sitio+'_FUA_IRIS.csv')\n",
    "\n",
    "polys = [shapely.wkt.loads(i) for i in iris[\"geometry\"]]\n",
    "iris_codes = gpd.GeoDataFrame(iris[[\"NOM_COM\",\"CODE_IRIS\",\"NOM_IRIS\"]],geometry=polys,crs=\"WGS84\")\n",
    "iris_codes = iris_codes.to_crs(epsg=3857)\n",
    "\n",
    "# Remove iris dataframe to avoid Memory error\n",
    "del iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create fake dataframe of L adds with a random price, random agency and a random CODE_IRIS\n",
    "L = 400\n",
    "\n",
    "census = pd.DataFrame(columns=['CODE_IRIS','price'])\n",
    "\n",
    "census[\"price\"] = np.random.randint(100000, 1000000, L)             # Random price with arbitrary range\n",
    "census[\"agent_code\"] = np.random.randint(0, 50, L)                  # Random agent code with arbitrary range\n",
    "census[\"CODE_IRIS\"] = np.random.choice(iris_codes[\"CODE_IRIS\"], L)  # Random CODE_IRIS from the list of CODE_IRIS\n",
    "census[\"id\"] = census.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid-census equivalence\n",
    "grid_cens = grid_census(iris_codes,cell_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "census[\"index_cell\"] = 0\n",
    "grid_cens[\"CODE_IRIS\"] = grid_cens[\"CODE_IRIS\"].astype(int)\n",
    "\n",
    "M = 1 # Number of stochastic networks to create\n",
    "\n",
    "for m in range(M):\n",
    "    \n",
    "    # First I merge the census with the grid_cens dataframe\n",
    "    for ind in range(len(census)):\n",
    "        CENS_ID = census.iloc[ind][\"CODE_IRIS\"]\n",
    "        df_ind = grid_cens[grid_cens[\"CODE_IRIS\"] == CENS_ID]\n",
    "        cumul = np.cumsum(df_ind[\"Area\"])\n",
    "        ir = np.random.rand()\n",
    "        index = np.where(cumul > ir)[0][0]\n",
    "        index_cell = df_ind[\"index_cell\"][df_ind.index[index]]\n",
    "        census.loc[ind,\"index_cell\"] = index_cell\n",
    "    \n",
    "    # This process is stochastic. Each realization of this process will be different.\n",
    "    # The network will be different for each realization.\n",
    "    \n",
    "    ## And now we create the network\n",
    "    cell = \"index_cell\"\n",
    "    df = census[[\"id\",\"agent_code\",cell]].groupby([\"agent_code\",cell]).count().reset_index()\n",
    "\n",
    "    # Build the bipartite graph\n",
    "    df[\"agent_code\"] = \"a_\" + df[\"agent_code\"].astype(str)\n",
    "\n",
    "    bank_celdas = list(np.unique(df[cell]))        # take the list of unique CP's\n",
    "    bank_agencies = list(df['agent_code'].unique())              # take the list of unique agencies\n",
    "\n",
    "    #%% Bipartita\n",
    "    B = nx.Graph()                                      # creates a Graph object from networkx\n",
    "    B.add_nodes_from(bank_agencies, bipartite=0)        # adds node for the layer of agencies\n",
    "    B.add_nodes_from(bank_celdas, bipartite=1)          # adds nodes for the layer of cp's\n",
    "    for idx, row in df.iterrows():        # adds link weights between one layer and the other\n",
    "        B.add_edge(row['agent_code'], row[cell], weight=row[\"id\"])\n",
    "\n",
    "    # Projection of the bipartite graph to the layer of agencies by influence\n",
    "    G_influence = nx.bipartite.generic_weighted_projected_graph(B, bank_celdas, weight_function = my_weight)\n",
    "    \n",
    "    # And save the stochastic network\n",
    "    nx.write_gpickle(G_influence,\"/home/david/IDEAL/STATIC/FRANCE/Network_\"+str(m)+\"_\"+sitio+\".pkl.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
